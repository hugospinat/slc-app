import os
import re
from typing import List

import pandas as pd
import tabula.io as tabula

from .base_processor import BaseProcessor


class Reg114Processor(BaseProcessor):
    """Processeur sp√©cialis√© pour l'extraction des donn√©es des PDF REG114 (tanti√®mes)"""

    def __init__(self):
        super().__init__()

    def extract_data_from_pdf(self, pdf_path: str) -> pd.DataFrame:
        """Extraire les donn√©es d'un PDF REG114 de mani√®re robuste"""
        try:
            self.log_info(f"Traitement du fichier REG114: {pdf_path}")
            self.log_info(f"üîÑ Extraction du PDF: {os.path.basename(pdf_path)}")

            # Extraire les tableaux du PDF avec lattice (m√™me m√©thode que REG010)
            dfs = tabula.read_pdf(
                pdf_path,
                pages="all",
                lattice=True,
                pandas_options={"header": None},
            )

            if not dfs:
                self.log_warning(f"Aucun tableau trouv√© dans {pdf_path}")
                return pd.DataFrame()

            # Combiner tous les DataFrames
            combined_df = pd.concat(dfs, ignore_index=True)
            self.log_info(f"üìä Donn√©es brutes extraites: {len(combined_df)} lignes, {combined_df.shape[1]} colonnes")

            if combined_df.empty or combined_df.shape[1] < 6:
                self.log_error(f"Format incorrect: {combined_df.shape[1]} colonnes (minimum 6 requis pour REG114)")
                return pd.DataFrame()

            # Nettoyer et structurer les donn√©es
            return self._process_extracted_data(combined_df, pdf_path)

        except Exception as e:
            self.log_error(f"Erreur dans {pdf_path}: {str(e)}")
            import traceback

            traceback.print_exc()
            return pd.DataFrame()

    def _process_extracted_data(self, df: pd.DataFrame, pdf_path: str) -> pd.DataFrame:
        """Traiter et nettoyer les donn√©es extraites du REG114 avec d√©tection des bases de r√©partition"""
        try:
            # Ajouter les m√©tadonn√©es de base
            df["fichier_source"] = os.path.basename(pdf_path)
            df["ligne_pdf"] = range(1, len(df) + 1)

            # Supprimer les lignes compl√®tement vides
            df = df.dropna(how="all")
            self.log_info(f"üìä Apr√®s suppression des lignes vides: {len(df)}")

            # NOUVEAU : Supprimer les lignes avec 7 colonnes ou plus non vides (doublons th√©oriques)
            df_filtre = self._filtrer_par_nombre_colonnes(df)

            # D√©tecter et traiter les bases de r√©partition
            df_with_bases = self._detect_and_group_by_bases(df_filtre)

            return df_with_bases

        except Exception as e:
            self.log_error(f"Erreur lors du traitement des donn√©es: {str(e)}")
            return pd.DataFrame()

    def _filtrer_par_nombre_colonnes(self, df: pd.DataFrame) -> pd.DataFrame:
        """Supprimer les lignes ayant 7 colonnes ou plus non vides (section √©carts th√©oriques)"""
        if df.empty:
            return df

        # Compter les colonnes non vides pour chaque ligne avec pandas
        # Exclure les colonnes ajout√©es (fichier_source, ligne_pdf)
        colonnes_donnees = [col for col in df.columns if col not in ["fichier_source", "ligne_pdf"]]

        # Cr√©er un masque pour les valeurs non vides (pas NaN et pas cha√Æne vide)
        mask_non_vide = df[colonnes_donnees].notna() & (df[colonnes_donnees].astype(str).str.strip() != "")

        # Compter les colonnes non vides par ligne
        nb_colonnes_par_ligne = mask_non_vide.sum(axis=1)

        # Garder seulement les lignes avec moins de 7 colonnes non vides
        mask_a_garder = nb_colonnes_par_ligne < 7

        nb_lignes_supprimees = (~mask_a_garder).sum()
        if nb_lignes_supprimees > 0:
            self.log_info(f"üîÑ Suppression doublons : {nb_lignes_supprimees} lignes avec 7+ colonnes supprim√©es")

        return df[mask_a_garder].copy()

    def _detect_and_group_by_bases(self, df: pd.DataFrame) -> pd.DataFrame:
        """D√©tecter les lignes de base de r√©partition et grouper les tanti√®mes"""

        try:
            base_repartition_rows = []
            tantieme_rows = []
            current_base = None
            codes_bases_vus = set()  # NOUVEAU : Tracker les codes de base d√©j√† vus

            # Regex pour d√©tecter les bases de r√©partition: code commen√ßant par une majuscule suivi de lettres/chiffres + " - "
            base_pattern = re.compile(r"([A-Z][A-Z0-9]+) - (.+)")

            for idx, row in df.iterrows():
                # V√©rifier si c'est une ligne de base de r√©partition
                first_col = str(row.iloc[0]) if not pd.isna(row.iloc[0]) else ""

                match = base_pattern.match(first_col.strip())
                if match:
                    # C'est une base de r√©partition
                    code_base = match.group(1)
                    nom_base = match.group(2).strip()

                    # NOUVEAU : V√©rifier si le code de base existe d√©j√†
                    if code_base in codes_bases_vus:
                        self.log_info(f"üîÑ Code de base dupliqu√© ignor√©: {code_base} (ligne {row['ligne_pdf']})")
                        # On ignore cette ligne et toutes les suivantes car on est dans la section doublons
                        break

                    # Ajouter le code √† la liste des codes vus
                    codes_bases_vus.add(code_base)

                    current_base = {
                        "code": code_base,
                        "nom": nom_base,
                        "cdc_concerne": None,  # √Ä d√©terminer si n√©cessaire
                        "fichier_source": row["fichier_source"],
                        "ligne_pdf": row["ligne_pdf"],
                        "type": "base_repartition",
                    }

                    base_repartition_rows.append(current_base)
                    self.log_info(f"üìã Base de r√©partition d√©tect√©e: {code_base} - {nom_base}")

                else:
                    # C'est une ligne de tanti√®me
                    if current_base is not None:
                        # V√©rifier si la ligne a suffisamment de colonnes pour √™tre un tanti√®me valide
                        if self._is_valid_tantieme_row(row):
                            tantieme_data = self._extract_tantieme_data(row, current_base)
                            if tantieme_data:  # V√©rifier que le dictionnaire n'est pas vide
                                tantieme_rows.append(tantieme_data)

            self.log_info(f"‚úÖ {len(base_repartition_rows)} bases de r√©partition d√©tect√©es")
            self.log_info(f"‚úÖ {len(tantieme_rows)} tanti√®mes extraits")

            # Cr√©er le DataFrame final avec les deux types de donn√©es
            all_rows = base_repartition_rows + tantieme_rows
            return pd.DataFrame(all_rows)

        except Exception as e:
            self.log_error(f"Erreur lors de la d√©tection des bases: {str(e)}")
            return df

    def _is_valid_tantieme_row(self, row) -> bool:
        """V√©rifier si une ligne est un tanti√®me valide"""

        try:
            # Au minimum : num√©ro UG (premi√®re colonne) non vide
            numero_ug = str(row.iloc[0]) if not pd.isna(row.iloc[0]) else ""
            if numero_ug.strip() == "" or " - " in numero_ug:
                return False

            return True
        except Exception:
            return False

    def _extract_tantieme_data(self, row, current_base):
        """Extraire les donn√©es d'un tanti√®me avec validation du montant"""
        try:
            # Colonnes attendues : UG, CA, d√©but, fin, tanti√®me, reliquat
            tantieme_data = {
                "type": "tantieme",
                "base_code": current_base["code"],
                "base_nom": current_base["nom"],
                "numero_ug": str(row.iloc[0]) if not pd.isna(row.iloc[0]) else "",
                "numero_ca": str(row.iloc[1]) if len(row) > 1 and not pd.isna(row.iloc[1]) else "",
                "debut_occupation": row.iloc[2] if len(row) > 2 else None,
                "fin_occupation": row.iloc[3] if len(row) > 3 else None,
                "tantieme": row.iloc[4] if len(row) > 4 else None,
                "reliquat": row.iloc[5] if len(row) > 5 else None,
                "fichier_source": row["fichier_source"],
                "ligne_pdf": row["ligne_pdf"],
            }

            # V√©rifier que le num√©ro UG n'est pas vide
            if tantieme_data["numero_ug"].strip() == "":
                return {}

            # Appliquer le m√™me filtre de montant que REG010 sur la colonne tanti√®me
            if not self._is_valid_tantieme_amount(tantieme_data["tantieme"]):
                self.log_debug(f"Ligne tanti√®me rejet√©e - Montant invalide: '{tantieme_data['tantieme']}'")
                return {}

            return tantieme_data

        except Exception as e:
            self.log_warning(f"Erreur lors de l'extraction tanti√®me ligne {row.get('ligne_pdf', '?')}: {str(e)}")
            return {}

    def _is_valid_tantieme_amount(self, montant) -> bool:
        """Valider le montant d'un tanti√®me avec le m√™me pattern que REG010"""
        import re

        if montant is None or pd.isna(montant):
            return False

        montant_str = str(montant).strip()
        if montant_str == "" or montant_str == "nan":
            return False

        # M√™me pattern que REG010 : montant d√©cimal avec 1 ou 2 chiffres apr√®s la virgule
        montant_pattern = r"^-?\d+(\.\d{1,2})?$"
        return bool(re.match(montant_pattern, montant_str))

    def _convert_numeric_fields(self, df: pd.DataFrame):
        """Convertir les champs num√©riques et dates"""
        try:
            # Conversion des champs num√©riques
            for col in ["tantieme", "reliquat"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

            # Conversion des dates (si format reconnu)
            for col in ["debut_occupation", "fin_occupation"]:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors="coerce", dayfirst=True)

        except Exception as e:
            self.log_warning(f"Erreur lors de la conversion des types: {str(e)}")

    def save_to_database(self, df: pd.DataFrame, controle_id: int) -> int:
        """Sauvegarder les bases de r√©partition et tanti√®mes en base de donn√©es"""
        if df.empty:
            self.log_warning("DataFrame vide, rien √† sauvegarder")
            return 0

        try:
            from sqlmodel import Session

            from models import BaseRepartition, Tantieme, engine

            saved_count = 0
            base_id_map = {}  # Mapping code_base -> base_id

            with Session(engine) as session:
                # D'abord sauvegarder les bases de r√©partition
                for _, row in df.iterrows():
                    if row.get("type") == "base_repartition":
                        base_repartition = BaseRepartition(
                            controle_id=controle_id,
                            code=row["code"],
                            nom=row["nom"],
                            cdc_concerne=row.get("cdc_concerne"),
                            fichier_source=row["fichier_source"],
                            ligne_pdf=int(row["ligne_pdf"]),
                        )
                        session.add(base_repartition)
                        session.flush()  # Pour obtenir l'ID
                        base_id_map[row["code"]] = base_repartition.id
                        saved_count += 1

                # Ensuite sauvegarder les tanti√®mes
                for _, row in df.iterrows():
                    if row.get("type") == "tantieme":
                        base_id = base_id_map.get(row["base_code"])
                        if base_id:
                            tantieme = Tantieme(
                                base_repartition_id=base_id,
                                numero_ug=str(row["numero_ug"]),
                                numero_ca=str(row["numero_ca"]),
                                debut_occupation=row.get("debut_occupation"),
                                fin_occupation=row.get("fin_occupation"),
                                tantieme=row.get("tantieme"),
                                reliquat=row.get("reliquat"),
                                fichier_source=row["fichier_source"],
                                ligne_pdf=int(row["ligne_pdf"]),
                            )
                            session.add(tantieme)
                            saved_count += 1

                session.commit()
                self.log_info(f"‚úÖ {saved_count} √©l√©ments sauvegard√©s en base (bases + tanti√®mes)")

            return saved_count

        except Exception as e:
            self.log_error(f"Erreur lors de la sauvegarde: {str(e)}")
            return 0

    def save_combined_to_csv(self, dataframes: List[pd.DataFrame], output_filename: str = "reg114.csv") -> str:
        """Sauvegarder le DataFrame fusionn√© en CSV"""
        try:
            if not dataframes:
                self.log_warning("Aucun DataFrame √† fusionner")
                return ""

            # Fusionner tous les DataFrames
            combined_df = pd.concat(dataframes, ignore_index=True)

            # Sauvegarder en CSV
            combined_df.to_csv(output_filename, index=False, encoding="utf-8")

            self.log_info(f"üìÅ DataFrame REG114 fusionn√© sauvegard√©: {output_filename}")
            self.log_info(f"üìä Total lignes sauvegard√©es: {len(combined_df)}")

            return output_filename

        except Exception as e:
            self.log_error(f"Erreur lors de la sauvegarde CSV: {str(e)}")
            return ""

    def process_reg114_files(self, pdf_files: List[str]) -> tuple[List[pd.DataFrame], List[str]]:
        """Traiter une liste de fichiers REG114"""
        dataframes = []
        processed_files = []

        for pdf_path in pdf_files:
            self.log_info(f"üîÑ Traitement de {os.path.basename(pdf_path)}")
            df = self.extract_data_from_pdf(pdf_path)

            if not df.empty:
                dataframes.append(df)
                processed_files.append(os.path.basename(pdf_path))

        return dataframes, processed_files
