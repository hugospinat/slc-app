import os
from typing import List, Tuple

import pandas as pd
import tabula.io as tabula
from sqlmodel import Session

from slc_app.models import (
    PosteReleve,
    SourceColPosteReleve,
    SourceColReleveIndividuel,
    ReleveIndividuel,
)
from slc_app.services.importer.ph.base_processor import BaseProcessor
from slc_app.utils.file_storage import save_file_from_path


class ParserEAU008C(BaseProcessor):
    """Processeur sp√©cialis√© pour l'extraction des donn√©es des PDF EAU008C (relev√©s individuels)"""

    def __init__(self):
        super().__init__()

    def _extract_data_from_pdf(self, pdf_path: str) -> pd.DataFrame:
        """Extraire les donn√©es d'un PDF EAU008C de mani√®re robuste"""
        try:
            self.log_info(f"Traitement du fichier EAU008C: {pdf_path}")
            self.log_info(f"üîÑ Extraction du PDF: {os.path.basename(pdf_path)}")

            # Extraire les tableaux du PDF avec lattice (m√™me m√©thode que REG114)
            dfs = tabula.read_pdf(
                pdf_path,
                pages="all",
                lattice=True,
                pandas_options={"header": None},
            )

            if not dfs:
                self.log_warning(f"Aucun tableau trouv√© dans {pdf_path}")
                raise ValueError(f"Aucun tableau trouv√© dans {pdf_path}")

            # Combiner tous les DataFrames
            combined_df = pd.concat(dfs, ignore_index=True)
            self.log_info(
                f"üìä Donn√©es brutes extraites: {len(combined_df)} lignes, {combined_df.shape[1]} colonnes"
            )

            # LOG: Afficher le contenu brut extrait du PDF pour debug
            self.log_info("[DEBUG] Structure compl√®te du DataFrame extrait:")
            self.log_info(f"[DEBUG] Colonnes: {list(combined_df.columns)}")
            self.log_info("[DEBUG] Premiers 15 lignes du DataFrame brut:")
            for i in range(min(15, len(combined_df))):
                ligne_str = " | ".join(
                    [str(val) if pd.notna(val) else "NaN" for val in combined_df.iloc[i]]
                )
                self.log_info(f"[DEBUG] Ligne {i}: {ligne_str}")

            if len(combined_df) > 15:
                self.log_info(f"[DEBUG] ... ({len(combined_df) - 15} autres lignes)")

            self.log_info("[DEBUG] Fin de l'affichage du DataFrame brut")

            if combined_df.empty or combined_df.shape[1] < 11:
                self.log_error(
                    f"Format incorrect: {combined_df.shape[1]} colonnes (minimum 11 requis pour EAU008C)"
                )
                raise ValueError(
                    f"Format incorrect: {combined_df.shape[1]} colonnes (minimum 11 requis pour EAU008C)"
                )

            return combined_df

        except Exception as e:
            self.log_error(f"Erreur dans {pdf_path}: {str(e)}")
            import traceback

            traceback.print_exc()
            raise ValueError(
                f"Erreur lors de l'extraction des donn√©es de {pdf_path}: {str(e)}"
            ) from e

    def _process_extracted_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Traiter et nettoyer les donn√©es extraites du EAU008C avec d√©tection des postes de relev√©"""
        # Nommer les colonnes selon le mod√®le de donn√©es EAU008C
        colonnes_eau008c = [
            "nom_poste",  # Colonne 0: nom du poste (propag√©)
            SourceColReleveIndividuel.NUMERO_UG,  # Colonne 1: N¬∞UG
            SourceColReleveIndividuel.NATURE_UG,  # Colonne 2: Nature UG
            SourceColReleveIndividuel.NUMERO_CA,  # Colonne 3: N¬∞CA
            SourceColReleveIndividuel.POINT_COMPTAGE,  # Colonne 4: Point de Comptage
            SourceColReleveIndividuel.NUMERO_SERIE_COMPTEUR,  # Colonne 5: N¬∞ S√©rie compteur
            SourceColReleveIndividuel.DATE_RELEVE,  # Colonne 6: Date du Relev√©
            SourceColReleveIndividuel.DATE_VALEUR,  # Colonne 7: Date de valeur
            SourceColReleveIndividuel.TYPE_RELEVE,  # Colonne 8: Type de Relev√©
            SourceColReleveIndividuel.OBSERVATIONS,  # Colonne 9: Observations
            SourceColReleveIndividuel.INDEX,  # Colonne 10: Index
            SourceColReleveIndividuel.EVOLUTION_INDEX,  # Colonne 11: Evolution Index
        ]
        colonnes_postes = [SourceColPosteReleve.NOM]

        try:
            self.log_info(f"[DEBUG] DataFrame initial: {df.shape}")
            self.log_info(f"[DEBUG] Colonnes: {df.columns.tolist()}")

            # Supprimer les lignes compl√®tement vides
            df = df.dropna(how="all")
            self.log_info(f"üìä Apr√®s suppression des lignes vides: {len(df)}")

            # Supprimer les lignes avec 12 colonnes ou plus non vides (doublons th√©oriques)
            df_filtre = self._filtrer_par_nombre_colonnes(df)
            self.log_info(f"[DEBUG] Apr√®s filtrage: {df_filtre.shape}")

            # Pattern pour d√©tecter les postes de relev√© (similaire aux bases de r√©partition)
            # Exemples: "EAU CHAUDE INDIVIDUELLE", "EAU FROIDE INDIVIDUELLE", "COMPTEUR DE CALORIE"
            pattern_poste = r"^([A-Z][A-Z\s]+)$"

            # Extraire les noms de postes
            self.log_info("[DEBUG] √âchantillon de donn√©es colonne 0 avant extraction:")
            sample_data = df_filtre.iloc[:10, 0].astype(str).tolist()
            for i, val in enumerate(sample_data):
                self.log_info(f"[DEBUG] Ligne {i}: '{val}'")

            postes_extraits = (
                df_filtre.iloc[:, 0].astype(str).str.extract(pattern_poste, expand=False)
            )
            self.log_info(f"[DEBUG] Postes extraits: {postes_extraits.count()} non-null")
            self.log_info("[DEBUG] √âchantillon postes_extraits:")
            self.log_info(f"{postes_extraits.dropna().head()}")

            # Propager le nom de poste vers le bas (forward fill) jusqu'au prochain poste
            noms_poste_propages = postes_extraits.ffill()
            self.log_info(f"[DEBUG] Noms propag√©s: {noms_poste_propages.count()} non-null")

            # Ajouter la colonne au DataFrame
            df_avec_poste = df.copy()
            df_avec_poste.insert(0, "nom_poste_actuel", noms_poste_propages)
            self.log_info(f"[DEBUG] DataFrame avec postes: {df_avec_poste.shape}")

            # Nous devons maintenant prendre seulement les colonnes n√©cessaires (12 premi√®res)
            # car le DataFrame original peut avoir plus de colonnes que pr√©vu
            nb_colonnes_attendues = len(colonnes_eau008c)
            if df_avec_poste.shape[1] > nb_colonnes_attendues:
                self.log_info(
                    f"[DEBUG] R√©duction de {df_avec_poste.shape[1]} √† {nb_colonnes_attendues} colonnes"
                )
                df_avec_poste = df_avec_poste.iloc[:, :nb_colonnes_attendues]

            df_avec_poste.columns = colonnes_eau008c
            self.log_info(f"[DEBUG] Colonnes finales: {df_avec_poste.columns.tolist()}")

            # Appliquer des validations plus strictes pour ne garder que les vrais relev√©s
            numero_ug_pattern = r"^\d+$"
            numero_ca_pattern = r"^\d+$"
            index_pattern = r"^\d+$"

            numero_ug_col = SourceColReleveIndividuel.NUMERO_UG
            numero_ca_col = SourceColReleveIndividuel.NUMERO_CA
            index_col = SourceColReleveIndividuel.INDEX

            self.log_info(
                f"[DEBUG] Validation stricte sur colonnes: {numero_ug_col}, {numero_ca_col}, {index_col}"
            )

            # Validation 1: Num√©ro UG doit √™tre num√©rique
            mask_numero_ug = df_avec_poste[numero_ug_col].astype(str).str.match(numero_ug_pattern)
            self.log_info(f"[DEBUG] Lignes avec UG valide: {mask_numero_ug.sum()}")

            # Validation 2: Num√©ro CA doit √™tre num√©rique
            mask_numero_ca = df_avec_poste[numero_ca_col].astype(str).str.match(numero_ca_pattern)
            self.log_info(f"[DEBUG] Lignes avec CA valide: {mask_numero_ca.sum()}")

            # Validation 3: Index doit √™tre num√©rique
            mask_index = df_avec_poste[index_col].astype(str).str.match(index_pattern)
            self.log_info(f"[DEBUG] Lignes avec Index valide: {mask_index.sum()}")

            # Combiner toutes les validations (ET logique)
            mask_complet = mask_numero_ug & mask_numero_ca & mask_index
            df_avec_poste = df_avec_poste[mask_complet]
            self.log_info(f"[DEBUG] Apr√®s validation compl√®te: {len(df_avec_poste)} lignes")

            # R√©cup√©rer les postes uniques extraits
            unique_postes_extraits = (
                postes_extraits.dropna().drop_duplicates().reset_index(drop=True)
            )
            self.log_info(
                f"[DEBUG] Postes uniques avant cr√©ation DataFrame: {unique_postes_extraits.shape}"
            )

            # V√©rifier que nous avons des postes valides
            if unique_postes_extraits.empty:
                self.log_warning("Aucun poste de relev√© trouv√© dans le document")
                # Cr√©er un DataFrame vide avec les bonnes colonnes
                postes_releve = pd.DataFrame(columns=colonnes_postes)
            else:
                self.log_info(f"[DEBUG] Premiers postes trouv√©s: {unique_postes_extraits.head()}")
                postes_releve = pd.DataFrame({SourceColPosteReleve.NOM: unique_postes_extraits})
                # V√©rifier s'il y a des valeurs nulles
                nulls_in_postes = postes_releve.isnull().sum()
                if nulls_in_postes.any():
                    self.log_warning(f"[DEBUG] Valeurs nulles dans les postes: {nulls_in_postes}")

            self.log_info(f"üìã Noms de postes propag√©s sur {len(df_avec_poste)} lignes")
            self.log_info(f"üìä {len(postes_releve)} postes de relev√© identifi√©s")

            return df_avec_poste, postes_releve

        except Exception as e:
            self.log_error(f"Erreur lors du traitement des donn√©es: {str(e)}")
            import traceback

            traceback.print_exc()
            # Re-raise l'erreur originale au lieu de la masquer
            raise e

    def _filtrer_par_nombre_colonnes(self, df: pd.DataFrame) -> pd.DataFrame:
        """Supprimer les lignes ayant 12 colonnes ou plus non vides (section √©carts th√©oriques)"""
        if df.empty:
            return df

        # Compter les colonnes non vides pour chaque ligne avec pandas
        # Exclure les colonnes ajout√©es (fichier_source, ligne_pdf)
        colonnes_donnees = [col for col in df.columns if col not in ["fichier_source", "ligne_pdf"]]

        # Approche enti√®rement vectoris√©e sans boucles
        df_subset = df[colonnes_donnees]

        # Masque pour les valeurs non-NaN
        mask_non_nan = df_subset.notna()

        # Convertir tout en string et cr√©er un masque pour les valeurs non vides
        df_str = df_subset.astype(str)
        mask_non_empty = (df_str != "") & (df_str != "nan") & (df_str != "None")

        # Combiner les deux masques
        mask_non_vide = mask_non_nan & mask_non_empty

        # Compter les colonnes non vides par ligne
        nb_colonnes_par_ligne = mask_non_vide.sum(axis=1)

        # Garder seulement les lignes avec moins de 12 colonnes non vides
        mask_a_garder = nb_colonnes_par_ligne < 12

        nb_lignes_supprimees = (~mask_a_garder).sum()
        if nb_lignes_supprimees > 0:
            self.log_info(
                f"üîÑ Suppression doublons : {nb_lignes_supprimees} lignes avec 12+ colonnes supprim√©es"
            )

        return df[mask_a_garder].copy()

    def _prepare_for_database(
        self, releves_df: pd.DataFrame, postes_df: pd.DataFrame, controle_id: int
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Pr√©parer les DataFrames pour la sauvegarde en base (sans session DB)"""
        if postes_df.empty or releves_df.empty:
            self.log_warning("DataFrames vides, rien √† pr√©parer")
            raise ValueError("‚ùå DataFrames vides, rien √† pr√©parer")

        # V√©rifier qu'il n'y a pas de valeurs nulles dans les colonnes critiques
        if postes_df[SourceColPosteReleve.NOM].isnull().any():
            self.log_error("Des noms de postes sont null, impossible de pr√©parer")
            self.log_info(
                f"[DEBUG] Postes_df avec nulls: {postes_df[postes_df[SourceColPosteReleve.NOM].isnull()]}"
            )
            raise ValueError("‚ùå Des noms de postes sont null, impossible de pr√©parer")

        try:
            # Pr√©parer les postes avec l'ID du contr√¥le
            postes_prepared = postes_df.copy()
            postes_prepared[SourceColPosteReleve.CONTROLE_ID] = controle_id
            
            # Pr√©parer les relev√©s avec conversion des types
            releves_prepared = releves_df.copy()
            
            # Convertir les colonnes num√©riques en string pour respecter le mod√®le
            string_columns = [
                SourceColReleveIndividuel.NUMERO_UG,
                SourceColReleveIndividuel.NUMERO_CA,
                SourceColReleveIndividuel.POINT_COMPTAGE,
                SourceColReleveIndividuel.NUMERO_SERIE_COMPTEUR,
                SourceColReleveIndividuel.TYPE_RELEVE,
                SourceColReleveIndividuel.OBSERVATIONS,
                SourceColReleveIndividuel.NATURE_UG,
            ]
            for col in string_columns:
                if col in releves_prepared.columns:
                    releves_prepared[col] = releves_prepared[col].fillna("").astype(str)
                    # Remplacer "nan" par cha√Æne vide pour les valeurs optionnelles
                    releves_prepared[col] = releves_prepared[col].replace("nan", "")

            self.log_info(f"üìã {len(postes_prepared)} postes de relev√© pr√©par√©s")
            self.log_info(f"üìä {len(releves_prepared)} relev√©s individuels pr√©par√©s")

            return releves_prepared, postes_prepared

        except Exception as e:
            raise ValueError(f"‚ùå Erreur lors de la pr√©paration des donn√©es: {str(e)}") from e
    
    def _save_to_database(
        self, releves_df: pd.DataFrame, postes_df: pd.DataFrame, session: Session
    ) -> Tuple[List[ReleveIndividuel], List[PosteReleve]]:
        """Sauvegarder les DataFrames pr√©par√©s en base de donn√©es avec une session courte"""
        try:
            # Cr√©er les objets PosteReleve
            poste_objects = PosteReleve.from_df(postes_df)
            session.add_all(poste_objects)
            session.commit()
            for p in poste_objects:
                session.refresh(p)

            # Cr√©er un mapping nom -> id pour les postes
            poste_id_map = {p.nom: p.id for p in poste_objects}
            releves_df_with_ids = releves_df.copy()
            releves_df_with_ids[SourceColReleveIndividuel.POSTE_RELEVE_ID] = releves_df_with_ids["nom_poste"].map(
                poste_id_map
            )

            # Cr√©er les objets ReleveIndividuel
            releve_objects = ReleveIndividuel.from_df(releves_df_with_ids)
            session.add_all(releve_objects)
            session.commit()
            for r in releve_objects:
                session.refresh(r)

            compteur_postes, compteur_releves = len(poste_objects), len(releve_objects)
            self.log_info(f"‚úÖ {compteur_postes} postes de relev√© sauvegard√©s en base")
            self.log_info(f"‚úÖ {compteur_releves} relev√©s individuels sauvegard√©s en base")

            return releve_objects, poste_objects

        except Exception as e:
            session.rollback()
            raise ValueError(f"‚ùå Erreur lors de la sauvegarde en base de donn√©es: {str(e)}") from e

    def process_eau008c(
        self, pdf_path: str, controle_id: int, savePath: str, session: Session
    ) -> Tuple[List[ReleveIndividuel], List[PosteReleve]]:
        """Traiter un fichier EAU008C complet"""
        self.log_info(f"üîÑ Traitement de {os.path.basename(pdf_path)}")
        data = self._extract_data_from_pdf(pdf_path)
        save_file_from_path(pdf_path, savePath, "eau008c.pdf")
        df_releves, df_postes = self._process_extracted_data(data)
        releves, postes_releve = self._save_to_database(df_releves, df_postes, controle_id, session)
        return releves, postes_releve
